{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f99b35",
   "metadata": {},
   "source": [
    "# 第四节 实战召回优化——大模型查询重写\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4fb8fa",
   "metadata": {},
   "source": [
    "上一节我们讲了召回的重要性，这一节内容我们将在代码层面实现优化召回率的其中一种方式——大模型查询重写。\n",
    "\n",
    "## 提升指标的优化策略\n",
    "\n",
    "做了这么久 RAG，我最大的感受就是——**没有完美的方案，都是在各种 trade-off 中找平衡**。不同的业务场景需要不同的优化重点，有时候甚至需要牺牲一些召回率来换取精确度。关键是要根据实际情况灵活调整，别死磕某一个指标。\n",
    "\n",
    "**查询重写**这块儿，我之前碰到过一个特别典型的案例——用户问\"这个怎么处理\"，你说这让系统怎么理解？后来我们就在查询重写上下了功夫。比如**在医疗领域的项目**里，用户说\"**头疼**\"，我们会自动扩展成\"**头痛、偏头痛、头部疼痛**\"这些同义词，召回率一下子就上去了。不过说真的，这招在**专业领域特别好使，通用场景下反而容易引入噪音**。\n",
    "\n",
    "那么如何从**代码层面实现大模型查询重写优化策略**呢？\n",
    "\n",
    "## 基于大模型的查询重写策略\n",
    "\n",
    "这种策略可以细分成**查询扩写**、**子问题查询**和**多步骤查询**三类。我们一点一点实现。\n",
    "\n",
    "### 查询扩写\n",
    "\n",
    "用户查询常常信息不足或存在歧义，影响检索效果。查询扩写通过同义词补充、上下文补全、模板转换等方式，使查询更清晰具体，从而提升准确率和召回率。我们还是基于 [cmrc2018](https://huggingface.co/datasets/LazyAGI/CMRC2018_Knowledge_Base/tree/main) 数据集做的 RAG 系统继续优化，上代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24219a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install lazyllm\n",
    "export QWEN_API_KEY='sk-这里填写你的key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f50063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询扩写\n",
    "# 导入必要的库\n",
    "import lazyllm\n",
    "import os\n",
    "from lazyllm import Document, ChatPrompter, Retriever\n",
    "\n",
    "# 查询改写提示词\n",
    "# 用于将用户的查询改写得更加清晰，不回答问题\n",
    "rewrite_prompt = \"你是一个查询改写助手，将用户的查询改写的更加清晰。\\\n",
    "                注意，你不需要对问题进行回答，只需要对原始问题进行改写。\\\n",
    "                下面是一个简单的例子：\\\n",
    "                输入：RAG\\\n",
    "                输出：为我介绍下RAG。\\\n",
    "                    \\\n",
    "                用户输入为：\"\n",
    "\n",
    "# AI问答助手提示词\n",
    "# 用于根据上下文和问题提供答案\n",
    "robot_prompt = \"你是一个友好的 AI 问答助手，你需要根据给定的上下文和问题提供答案。\\\n",
    "                根据以下资料回答问题：\\\n",
    "                {context_str} \\n\"\n",
    "\n",
    "# 加载文档库，定义检索器和在线大模型\n",
    "# 创建文档对象，指定数据集路径\n",
    "documents = Document(dataset_path=\"./data_kb\")  # 请在 dataset_path 传入数据集绝对路径\n",
    "# 创建检索器，指定文档、分组名、相似度算法和返回结果数量\n",
    "retriever = Retriever(\n",
    "    doc=documents, group_name=\"CoarseChunk\", similarity=\"bm25_chinese\", topk=3\n",
    ")  # 定义检索组件\n",
    "# 创建在线大模型对象，指定来源、模型和API密钥\n",
    "llm = lazyllm.OnlineChatModule(\n",
    "    source=\"qwen\",\n",
    "    model=\"qwen-plus-latest\",\n",
    "    api_key=os.getenv(\"QWEN_API_KEY\"),\n",
    ")  # 调用大模型\n",
    "\n",
    "# 设置大模型的提示词\n",
    "llm.prompt(ChatPrompter(instruction=robot_prompt, extra_keys=[\"context_str\"]))\n",
    "# 定义查询问题\n",
    "query = \"MIT OpenCourseWare是啥？\"\n",
    "\n",
    "# 创建查询改写器并改写查询\n",
    "query_rewriter = llm.share(ChatPrompter(instruction=rewrite_prompt))\n",
    "query = query_rewriter(query)\n",
    "print(f\"改写后的查询：\\n{query}\")\n",
    "\n",
    "# 使用检索器检索相关文档\n",
    "doc_node_list = retriever(query=query)\n",
    "\n",
    "# 将查询和召回节点中的内容组成dict，作为大模型的输入\n",
    "# 构造大模型输入参数，包括查询和上下文\n",
    "res = llm(\n",
    "    {\n",
    "        \"query\": query,\n",
    "        \"context_str\": \"\".join([node.get_content() for node in doc_node_list]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# 输出大模型的回答\n",
    "print(\"\\n回答: \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701953cf",
   "metadata": {},
   "source": [
    "### 子问题查询\n",
    "\n",
    "将主问题拆解为多个子问题，有助于系统从不同角度理解问题并生成更全面的答案。**每个子问题单独检索，最后合并答案**。适用于**抽象或复合型查询**，但需注意处理性能开销。我们配合前面的查询扩写进行优化，上代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbace4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 子问题查询\n",
    "# 导入必要的库\n",
    "import lazyllm\n",
    "from lazyllm import Document, ChatPrompter, Retriever\n",
    "\n",
    "# 查询改写提示词\n",
    "# 用于将用户的查询改写得更加清晰，不回答问题\n",
    "rewrite_prompt = \"你是一个查询改写助手，将用户的查询改写的更加清晰。\\\n",
    "                注意，你不需要对问题进行回答，只需要对原始问题进行改写。\\\n",
    "                下面是一个简单的例子：\\\n",
    "                输入：RAG\\\n",
    "                输出：为我介绍下RAG。\\\n",
    "                    \\\n",
    "                用户输入为：\"\n",
    "\n",
    "# 子问题拆分提示词\n",
    "# 用于将主问题拆解为多个子问题，有助于系统从不同角度理解问题\n",
    "sub_query_prompt = \"你是一个查询重写助手，如果用户查询较为抽象或笼统，将其分解为多个角度的具体问题。\\\n",
    "                  注意，你不需要进行回答，只需要根据问题的字面进行子问题拆分，输出不要超过3条。\\\n",
    "                  下面是一个简单的例子：\\\n",
    "                  输入：RAG是什么？\\\n",
    "                  输出：RAG的定义是什么？\\\n",
    "                       RAG是什么领域内的名词？\\\n",
    "                       RAG有什么特点？\"\n",
    "\n",
    "# AI问答助手提示词\n",
    "# 用于根据上下文和问题提供答案\n",
    "robot_prompt = \"你是一个友好的 AI 问答助手，你需要根据给定的上下文和问题提供答案。\\\n",
    "                根据以下资料回答问题：\\\n",
    "                {context_str} \\n\"\n",
    "\n",
    "# 加载文档库，定义检索器和在线大模型\n",
    "# 创建文档对象，指定数据集路径\n",
    "documents = Document(dataset_path=\"./data_kb\")  # 请在 dataset_path 传入数据集绝对路径\n",
    "# 创建检索器，指定文档、分组名、相似度算法和返回结果数量\n",
    "retriever = Retriever(\n",
    "    doc=documents, group_name=\"CoarseChunk\", similarity=\"bm25_chinese\", topk=3\n",
    ")  # 定义检索组件\n",
    "# 创建在线大模型对象，指定来源、模型和API密钥\n",
    "llm = lazyllm.OnlineChatModule(\n",
    "    source=\"qwen\",\n",
    "    model=\"qwen-plus-latest\",\n",
    "    api_key=os.getenv(\"QWEN_API_KEY\"),\n",
    ")  # 调用大模型\n",
    "\n",
    "# 设置大模型的提示词\n",
    "llm.prompt(ChatPrompter(instruction=robot_prompt, extra_keys=[\"context_str\"]))\n",
    "# 定义查询问题\n",
    "query = \"MIT OpenCourseWare是啥？\"\n",
    "\n",
    "# 创建查询改写器并改写查询\n",
    "query_rewriter = llm.share(ChatPrompter(instruction=rewrite_prompt))\n",
    "query = query_rewriter(query)\n",
    "print(f\"改写后的查询：\\n{query}\")\n",
    "\n",
    "# 创建子问题生成器\n",
    "sub_query_generator = llm.share(ChatPrompter(instruction=sub_query_prompt))\n",
    "sub_queries_str = sub_query_generator(query)\n",
    "sub_queries = [q.strip() for q in sub_queries_str.split('\\n') if q.strip()]\n",
    "print(f\"拆分的子问题：\\n{sub_queries}\")\n",
    "\n",
    "# 对每个子问题进行检索并合并结果\n",
    "all_context = []\n",
    "for sub_query in sub_queries:\n",
    "    print(f\"\\n正在处理子问题: {sub_query}\")\n",
    "    # 使用检索器检索与子问题相关的文档\n",
    "    doc_node_list = retriever(query=sub_query)\n",
    "    # 将检索到的文档内容添加到上下文列表中\n",
    "    for node in doc_node_list:\n",
    "        all_context.append(node.get_content())\n",
    "\n",
    "# 去重并合并上下文\n",
    "# 使用set去除重复内容，然后合并为一个字符串\n",
    "unique_context = list(set(all_context))\n",
    "context_str = \"\".join(unique_context)\n",
    "print(f\"\\n合并后的上下文长度: {len(context_str)}\")\n",
    "\n",
    "# 将查询和合并后的上下文作为大模型的输入\n",
    "# 构造输入字典，包含查询和上下文\n",
    "res = llm(\n",
    "    {\n",
    "        \"query\": query,\n",
    "        \"context_str\": context_str,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 输出大模型的回答\n",
    "print(\"\\n回答: \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ab081",
   "metadata": {},
   "source": [
    "这次的输出会多一段处理的子问题以及合并之后的上下文长度：\n",
    "\n",
    "```bash\n",
    "正在处理子问题: MIT OpenCourseWare 由哪个机构推出？\n",
    "\n",
    "正在处理子问题: MIT OpenCourseWare 提供哪些类型的课程资源？\n",
    "\n",
    "合并后的上下文长度: 3475\n",
    "```\n",
    "\n",
    "### 多步骤查询\n",
    "\n",
    "将**复杂问题拆解为连续的多个步骤**，每一步依赖上一步的答案推进。**适合需要多轮推理的场景**，能有效提升对复杂问题的处理能力。我们配合前面的查询扩写进行优化，上代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd21285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分步骤查询\n",
    "# 导入必要的库\n",
    "import lazyllm\n",
    "import os\n",
    "from lazyllm import Document, ChatPrompter, Retriever\n",
    "\n",
    "# prompt设计\n",
    "# 查询改写提示词，用于将用户的查询改写得更加清晰，不回答问题\n",
    "rewrite_prompt = \"你是一个查询改写助手，将用户的查询改写的更加清晰。\\\n",
    "                注意，你不需要对问题进行回答，只需要对原始问题进行改写。\\\n",
    "                下面是一个简单的例子：\\\n",
    "                输入：RAG\\\n",
    "                输出：为我介绍下RAG。\\\n",
    "                    \\\n",
    "                用户输入为：\"\n",
    "\n",
    "# 判别提示词，用于判断某个回答能否解决对应的问题\n",
    "judge_prompt = \"你是一个判别助手，用于判断某个回答能否解决对应的问题。如果回答可以解决问题，则输出True，否则输出False。\\\n",
    "                注意，你的输出只能是True或者False。不要带有任何其他输出。\\\n",
    "                当前回答为{context_str} \\n\"\n",
    "\n",
    "# AI问答助手提示词，用于根据上下文和问题提供答案\n",
    "robot_prompt = \"你是一个友好的 AI 问答助手，你需要根据给定的上下文和问题提供答案。\\\n",
    "                根据以下资料回答问题：\\\n",
    "                {context_str} \\n\"\n",
    "\n",
    "# 加载文档库，定义检索器和在线大模型\n",
    "# 创建文档对象，指定数据集路径\n",
    "documents = Document(dataset_path=\"./data_kb\")  # 请在 dataset_path 传入数据集绝对路径\n",
    "# 创建检索器，指定文档、分组名、相似度算法和返回结果数量\n",
    "retriever = Retriever(\n",
    "    doc=documents, group_name=\"CoarseChunk\", similarity=\"bm25_chinese\", topk=3\n",
    ")  # 定义检索组件\n",
    "# 创建在线大模型对象，指定来源、模型和API密钥\n",
    "llm = lazyllm.OnlineChatModule(\n",
    "    source=\"qwen\",\n",
    "    model=\"qwen-plus-latest\",\n",
    "    api_key=os.getenv(\"QWEN_API_KEY\"),\n",
    ")  # 调用大模型\n",
    "\n",
    "# 创建查询改写器\n",
    "rewrite_robot = llm.share(ChatPrompter(instruction=rewrite_prompt))\n",
    "\n",
    "# 创建AI问答助手\n",
    "robot = llm.share(ChatPrompter(instruction=robot_prompt, extra_keys=[\"context_str\"]))\n",
    "\n",
    "# 创建判别助手\n",
    "judge_robot = llm.share(ChatPrompter(instruction=judge_prompt, extra_keys=[\"context_str\"]))\n",
    "\n",
    "# 定义查询问题\n",
    "query = \"MIT OpenCourseWare是啥？\"\n",
    "\n",
    "# 分步骤查询逻辑\n",
    "LLM_JUDGE = False\n",
    "while LLM_JUDGE is not True:\n",
    "    # 执行查询重写\n",
    "    query_rewrite = rewrite_robot(query)\n",
    "    print(f\"\\n重写的查询：{query_rewrite}\")\n",
    "\n",
    "    # 获取重写后的查询结果\n",
    "    doc_node_list = retriever(query_rewrite)\n",
    "    # 使用AI问答助手生成回答\n",
    "    res = robot({\"query\": query_rewrite, \"context_str\": \"\\n\".join([node.get_content() for node in doc_node_list])})\n",
    "\n",
    "    # 判断当前回复是否能满足query要求\n",
    "    LLM_JUDGE = bool(judge_robot({\"query\": query, \"context_str\": res}))\n",
    "    print(f\"\\nLLM判断结果：{LLM_JUDGE}\")\n",
    "\n",
    "# 打印最终结果\n",
    "print(\"\\n最终回复: \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077a33a",
   "metadata": {},
   "source": [
    "这次的输出会多一段每一步骤的回复能否满足查询的要求，由`LLM_JUDGE`这个变量决定：\n",
    "\n",
    "```bash\n",
    "LLM判断结果：True\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
