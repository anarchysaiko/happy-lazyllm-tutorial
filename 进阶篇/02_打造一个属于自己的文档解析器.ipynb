{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "681ca6d8",
   "metadata": {},
   "source": [
    "# 第二节 打造一个属于自己的文档解析器\n",
    "\n",
    "说实话，第一次接触 RAG 系统的时候，我被文档格式这个问题折腾得够呛。\n",
    "\n",
    "你想想看，一个系统要从成千上万的文档里找信息，这些文档什么格式都有——PDF、Word、PPT，甚至还有一些奇奇怪怪的格式。这就像是**你要从一个杂物间里找东西，但每个盒子的开法都不一样**。\n",
    "\n",
    "这时候**文档解析器 Reader** 就派上用场了。它基本上就是个\"万能钥匙\"，专门负责打开这些不同格式的文档，然后把内容整理成系统能理解的样子。我之前用 LazyLLM 的时候发现，它其实已经内置了挺多格式的支持——从常见的 PDF、Word 甚至音视频文件都有。记得有一次项目里需要处理韩文的 hwp 文件，当时还挺惊喜的，居然默认就支持了。\n",
    "\n",
    "但问题来了，总有**些时候默认的 Reader 满足不了需求**。比如说，**html 网页也是一种文档格式**，或者你想要 Reader 输出的内容**按照特定的方式**组织。这种情况下，自己写个 Reader 就很有必要了。\n",
    "\n",
    "这篇教程就是教你怎么在 LazyLLM 里面自定义 Reader 的。说白了，就是告诉你怎么给系统加个新的\"钥匙\"，让它能处理那些原本处理不了的文档。学会这个之后，你基本上就能搭建一个能处理各种奇葩格式的 RAG 应用了——这在实际工作中还挺有用的，特别是当你面对一些不太兼容的文档格式时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81cd406",
   "metadata": {},
   "source": [
    "# Reader 是什么——阅读 document 组件内容的工具\n",
    "\n",
    "回忆一下 RAG 的基本流程，用户一个问题进来，Retriever 要去找相关内容对吧？但问题是——它从哪儿找？\n",
    "\n",
    "你的知识库里（**之前提到的 document 组件**）什么都有，PDF 文档、Excel 表格、甚至还有些老系统导出的 XML 文件。\n",
    "\n",
    "Reader 这个模块就像是个**翻译官**，不管你的原始资料是什么格式——可能是整整齐齐存在数据库里的结构化数据，也可能是乱七八糟的 Word 文档——Reader 都得把它们\"翻译\"成检索模块能理解的统一格式。\n",
    "\n",
    "那问题来了，**如何实现一个最小的 reader 组件呢**？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879ba7a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install lazyllm\n",
    "\n",
    "export QWEN_API_KEY='sk-这里填写你的key'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46664841",
   "metadata": {},
   "source": [
    "## 实现 Reader 组件\n",
    "\n",
    "话说上回我们用[cmrc2018](https://huggingface.co/datasets/LazyAGI/CMRC2018_Knowledge_Base/tree/main) 数据集搭了个知识库（就是那个`data_kb`文件夹），今天咱们来看看怎么用 Reader 组件把里面的内容读出来。\n",
    "\n",
    "废话不多说，直接上代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aedab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 lazyllm.tools.rag 模块中的 Document 类\n",
    "from lazyllm.tools.rag import Document\n",
    "\n",
    "# 创建 Document 实例，指定数据集路径为当前目录下的 data_kb 文件夹\n",
    "doc = Document(dataset_path=\"./data_kb\")\n",
    "\n",
    "# 使用 Document 实例的内部 reader 加载指定文件的数据\n",
    "# 这里加载的是 data_kb 文件夹中的 part_1.txt 文件\n",
    "data = doc._impl._reader.load_data(input_files=[\"./data_kb/part_1.txt\"])\n",
    "\n",
    "# 打印加载的数据内容，输出结果为data: [<Node id=2d75ea15-f278-4f78-98ba-41da7f442c81>]\n",
    "print(f\"data: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc9609",
   "metadata": {},
   "source": [
    "可能细心的你会问：\n",
    "\n",
    "> 等等，这输出怎么是个 Node 对象？我的文本内容呢？\n",
    "\n",
    "别急，得这么取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"text: {data[0].text}\")\t#这里输出结果就是 part_1.txt 的文件内容了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e0916",
   "metadata": {},
   "source": [
    "说白了，Reader 就是把文档内容解析出来，然后包装成统一格式，方便后续模块使用。挺简单的对吧？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa1c644",
   "metadata": {},
   "source": [
    "## 给 Reader 组件加上 html 解析功能\n",
    "\n",
    "这时候问题来了 - **要是遇到 LazyLLM 不支持的文档格式咋办**？比如 HTML 文件？\n",
    "\n",
    "我手头有个维基百科的 HTML 页面，用默认 Reader 一读，好家伙，直接把 HTML 源码都打印出来了：\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html\n",
    "  class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\"\n",
    "  lang=\"zh-Hans-CN\"\n",
    "  dir=\"ltr\"\n",
    ">\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <title>网站 - 维基百科，自由的百科全书</title>\n",
    "    ...\n",
    "  </head>\n",
    "</html>\n",
    "```\n",
    "\n",
    "这谁看得懂啊！我要的是纯文本内容，不是这堆标签，要是能打印出去掉标签的格式该多好。\n",
    "\n",
    "我们可以**定义一个我们自己的 Reader**，因为读取 html 文件内容通常需要`BeautifulSoup`这个模块，所以我们先使用 `BeautifulSoup` 包**把 HTML 标签剥掉**，**把解析出来的文本包装成 LazyLLM 能识别的格式**，通过 DocNode 类实例化成一个 node 对象，然后再以 list 的格式返回。代码这样写："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_text(html_input, extra_info=None):\n",
    "    \"\"\"\n",
    "    清理HTML文本，去除标签和多余的空白字符，并将其封装为DocNode对象\n",
    "    :param html_input: 包含HTML标签的原始文本或文件路径\n",
    "    :param extra_info: 额外的元数据信息\n",
    "    :return: 包含DocNode对象的列表\n",
    "    \"\"\"\n",
    "    # 如果输入是路径，则读取文件内容\n",
    "    if isinstance(html_input, Path):\n",
    "        with open(html_input, \"r\", encoding=\"utf-8\") as f:\n",
    "            html_content = f.read()\n",
    "    else:\n",
    "        html_content = html_input\n",
    "\n",
    "    # 使用 BeautifulSoup 去除 HTML 标签，只保留纯文本内容\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    clean_text = soup.get_text()\n",
    "\n",
    "    # 去除多余的换行符和空白字符\n",
    "    clean_text = re.sub(r\"\\n+\", \"\\n\", clean_text)  # 将多个连续的换行符替换为单个换行符\n",
    "    clean_text = re.sub(r\"\\s+\", \" \", clean_text)  # 将多个连续的空白字符替换为单个空格\n",
    "\n",
    "    # 创建 DocNode 对象并返回列表\n",
    "    node = DocNode(text=clean_text, metadata=extra_info or {})\n",
    "    return [node]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8616816",
   "metadata": {},
   "source": [
    "代码主要功能都在`clean_html_text`这个函数内，输入的内容是`html`文件，输出的内容是**保留纯文本内容，同时去掉多余的换行符和空白字符的 DocNode 列表**。\n",
    "\n",
    "聪明的你可能会问了：\n",
    "\n",
    "> 我先运行默认 Reader 解析，再去除标签和换行符不也一样吗？\n",
    "\n",
    "这里有个巧妙的设计 - 注册机制。**直接看代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50170912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_text(html_input, extra_info=None):\n",
    "    \"\"\"\n",
    "    清理HTML文本，去除标签和多余的空白字符，并将其封装为DocNode对象\n",
    "    :param html_input: 包含HTML标签的原始文本或文件路径\n",
    "    :param extra_info: 额外的元数据信息\n",
    "    :return: 包含DocNode对象的列表\n",
    "    \"\"\"\n",
    "    # 如果输入是路径，则读取文件内容\n",
    "    if isinstance(html_input, Path):\n",
    "        with open(html_input, \"r\", encoding=\"utf-8\") as f:\n",
    "            html_content = f.read()\n",
    "    else:\n",
    "        html_content = html_input\n",
    "\n",
    "    # 使用 BeautifulSoup 去除 HTML 标签，只保留纯文本内容\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    clean_text = soup.get_text()\n",
    "\n",
    "    # 去除多余的换行符和空白字符\n",
    "    clean_text = re.sub(r\"\\n+\", \"\\n\", clean_text)  # 将多个连续的换行符替换为单个换行符\n",
    "    clean_text = re.sub(r\"\\s+\", \" \", clean_text)  # 将多个连续的空白字符替换为单个空格\n",
    "\n",
    "    # 创建 DocNode 对象并返回列表\n",
    "    node = DocNode(text=clean_text, metadata=extra_info or {})\n",
    "    return [node]\n",
    "\n",
    "\n",
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "   \t# 创建 Document 实例，指定数据集路径\n",
    "    doc = Document(dataset_path=\"./data_kb\")\n",
    "    # 为 Document 实例添加自定义 HTML reader\n",
    "    doc.add_reader(\"*.html\", clean_html_text)\n",
    "\n",
    "    # 使用注册的 reader 加载指定的 HTML 文件\n",
    "    data = doc._impl._reader.load_data(input_files=[\"./data_kb/网站.html\"])\n",
    "    # 打印加载的数据对象\n",
    "    print(f\"data: {data}\")\n",
    "    # 打印第一个数据节点的文本内容\n",
    "    print(f\"text: {data[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7abc3e",
   "metadata": {},
   "source": [
    "`add_reader`这个方法太好用了，一行代码就把自定义解析器注册进去了。以后不管遇到多少 HTML 文件，都能自动用这个解析器处理，省了不少事。\n",
    "说实话，LazyLLM 这个扩展机制设计得挺灵活的。你要是有 PDF、Word 或者其他格式的文档，照葫芦画瓢写个解析函数，注册一下就能用了。这比每次都单独写解析代码方便多了。\n",
    "\n",
    "到这里，**我们成功打造了一个 html 的文档解析器**。\n",
    "\n",
    "那我们能不能把这个自定义的解析器用到 RAG 系统里呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681bb8a",
   "metadata": {},
   "source": [
    "## 基于 html 解析器的 RAG 系统\n",
    "\n",
    "这里我们在本地**新建一个数据集**，文件夹名叫`rag_data`，然后往这个文件夹里面放入你的 html 文件，**这个过程相当于构建你的知识库**。我的文件夹里面放了这些：\n",
    "\n",
    "```bash\n",
    "rag_data/\n",
    "├── hongkong.html\n",
    "├── housing.html\n",
    "├── national_health_insurance_administration.html\n",
    "└── 网站.html\n",
    "```\n",
    "\n",
    "准备好知识库之后，然后就是写代码了。关键是要把 HTML 转换成纯文本，这个处理过程上面已经实现过了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b24618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyllm.tools.rag import Document, DocNode, Retriever\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import lazyllm\n",
    "from lazyllm import bind\n",
    "\n",
    "\n",
    "def clean_html_text(html_input, extra_info=None):\n",
    "    \"\"\"\n",
    "    清理HTML文本，去除标签和多余的空白字符，并将其封装为DocNode对象\n",
    "    :param html_input: 包含HTML标签的原始文本或文件路径\n",
    "    :param extra_info: 额外的元数据信息\n",
    "    :return: 包含DocNode对象的列表\n",
    "    \"\"\"\n",
    "    # 如果输入是路径，则读取文件内容\n",
    "    if isinstance(html_input, Path):\n",
    "        with open(html_input, \"r\", encoding=\"utf-8\") as f:\n",
    "            html_content = f.read()\n",
    "    else:\n",
    "        html_content = html_input\n",
    "\n",
    "    # 使用 BeautifulSoup 去除 HTML 标签，只保留纯文本内容\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    clean_text = soup.get_text()\n",
    "\n",
    "    # 去除多余的换行符和空白字符\n",
    "    clean_text = re.sub(r\"\\n+\", \"\\n\", clean_text)  # 将多个连续的换行符替换为单个换行符\n",
    "    clean_text = re.sub(r\"\\s+\", \" \", clean_text)  # 将多个连续的空白字符替换为单个空格\n",
    "\n",
    "    # 创建 DocNode 对象并返回列表\n",
    "    node = DocNode(text=clean_text, metadata=extra_info or {})\n",
    "    return [node]\n",
    "\n",
    "\n",
    "# 创建 Document 实例，指定数据集路径\n",
    "rag_data_path = \"./rag_data\"\n",
    "documents = Document(dataset_path=rag_data_path)\n",
    "# 为 Document 实例添加自定义 HTML reader\n",
    "documents.add_reader(\"*.html\", clean_html_text)\n",
    "\n",
    "# 定义提示词模板，告诉模型扮演AI问答助手的角色\n",
    "prompt = \"You will act as an AI question-answering assistant and complete a dialogue task. \\\n",
    "          In this task, you need to provide your answers based on the given context and questions.\"\n",
    "\n",
    "# 创建一个处理流程(pipeline)，包含检索和生成两个主要步骤\n",
    "with lazyllm.pipeline() as ppl:\n",
    "    # 检索组件定义：用于从知识库中检索相关信息\n",
    "    # doc: 指定文档对象\n",
    "    # group_name: 指定文档分组方式为\"CoarseChunk\"(粗粒度分块)\n",
    "    # similarity: 使用\"bm25_chinese\"算法计算相似度，适合中文检索\n",
    "    # topk: 返回最相关的3个结果\n",
    "    ppl.retriever = Retriever(\n",
    "        doc=documents, group_name=\"CoarseChunk\", similarity=\"bm25_chinese\", topk=3\n",
    "    )\n",
    "\n",
    "    # 格式化组件：将检索到的节点内容和查询问题格式化为适合LLM处理的格式\n",
    "    # nodes: 检索到的文档节点\n",
    "    # query: 用户的查询问题\n",
    "    # context_str: 将所有检索到的节点内容拼接成字符串\n",
    "    ppl.formatter = (\n",
    "        lambda nodes, query: {\n",
    "            \"query\": query,\n",
    "            \"context_str\": \"\".join([node.get_content() for node in nodes]),\n",
    "        }\n",
    "    ) | bind(query=ppl.input)\n",
    "\n",
    "    # 生成组件定义：使用在线大语言模型进行回答生成\n",
    "    # source: 指定模型来源为\"qwen\"(通义千问)\n",
    "    # model: 指定具体模型为\"qwen-plus-latest\"\n",
    "    # api_key: 设置API密钥用于访问模型\n",
    "    # prompt: 使用ChatPrompter包装提示词模板，并添加额外的context_str上下文信息\n",
    "    ppl.llm = lazyllm.OnlineChatModule(\n",
    "        source=\"qwen\",\n",
    "        model=\"qwen-plus-latest\",\n",
    "        api_key=os.getenv(\"QWEN_API_KEY\"),\n",
    "    ).prompt(lazyllm.ChatPrompter(instruction=prompt, extra_keys=[\"context_str\"]))\n",
    "\n",
    "# 启动Web服务模块，提供图形化界面进行交互\n",
    "# ppl: 传入上面定义的处理流程\n",
    "# port: 指定服务运行端口为23466\n",
    "lazyllm.WebModule(ppl, port=23466).start().wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ae4d4",
   "metadata": {},
   "source": [
    "大致执行流程可以参考**注释**：首先定义了一个 `clean_html_text` 函数，用于清理 HTML 文本，去除标签和多余的空白字符，并将其封装为 `DocNode` 对象。然后，它创建了一个 `Document` 实例，指定 `rag_data` 目录作为数据源，并为该实例添加了自定义的 HTML 读取器。接下来，脚本定义了一个处理流程（pipeline），包含检索和生成两个主要步骤。检索组件使用 `bm25_chinese` 算法从知识库中检索最相关的 3 个结果。生成组件则使用通义千问（Qwen）大语言模型，根据检索到的上下文和用户问题生成回答。最后，脚本启动了一个 Web 服务模块，提供图形化界面进行交互，用户可以通过 [http://127.0.0.1:23466](http://127.0.0.1:23466) 与系统进行交互。\n",
    "\n",
    "系统跑起来之后，我测试了一下。比如我问\"全国住房城乡建设工作会议的主要内容\"，系统准确地从`national_health_insurance_administration.html`里找到了相关内容。看截图就知道，不仅找对了文件，还把关键信息都提取出来了。\n",
    "\n",
    "![image](../assets/image-20250905181440-tn9t111.png \"知识库问题答案来源\")\n",
    "\n",
    "![image](../assets/image-20250905181548-n2qitrg.png \"答案成功获取\")\n",
    "\n",
    "其实整个过程最让我惊喜的是，处理后的效果比我预期的好很多。原本那些杂乱的 HTML 页面，经过清理后变成了结构化的知识库，查询起来特别顺畅。\n",
    "\n",
    "不过也有些不足的地方。比如有些**HTML 里面的表格数据**，用`get_text()`提取后格式会乱掉。可能需要针对表格单独处理。还有就是图片信息会丢失，如果 HTML 里有重要的图表，目前这个方案是处理不了的。\n",
    "\n",
    "总的来说，这个方案对于处理大量 HTML 文档还是挺实用的。特别是你有很多网页形式的资料需要整理成知识库时，这个方法能省不少事。关键是`BeautifulSoup`和`LazyLLM`配合得很好，几十行代码就能搭建一个可用的系统。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
