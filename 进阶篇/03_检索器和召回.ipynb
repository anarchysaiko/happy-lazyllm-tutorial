{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77de088c",
   "metadata": {},
   "source": [
    "# 第三节 检索器与召回\n",
    "\n",
    "之前的内容里，我们讲了 RAG 的三大基本组件：**检索器**（Retriever）、**外部文档**（Document）和**大语言模型**（LLM）。RAG 这玩意儿，说白了就是让**大模型不再瞎编，通过检索外部知识来生成靠谱的答案**。\n",
    "\n",
    "但问题是，**怎么知道你搭的 RAG 系统到底行不行？** 这就像考试，得有个评分标准。\n",
    "\n",
    "我觉得可以从这么几个角度来看：\n",
    "\n",
    "首先是**检索**（Retriever）这块儿。你想啊，如果检索回来的文档压根就不相关，那后面生成啥都白搭。衡量检索文档的有效性有一个指标——**召回率。** 如果召回率低 **，** 或者说无限趋近于 0 **，** 系统基本就退化成了纯大模型回答，那效果...简直没眼看。所以召回率这个指标特别关键，它直接决定了**你能给生成模型提供多少有用的\"弹药\"** 。\n",
    "\n",
    "但光召回还不够。有时候你**检索回来一大堆文档，里面可能就几句话是真正有用的，其余都是噪音**。这就像你找资料写论文，翻了十本书，真正能用的可能就那么几段。\n",
    "\n",
    "所以还得看**上下文相关性**，也就是 RAGAS（RAG Assessment，RAG 评估的缩写)里说的**Context Relevance**。这个指标能告诉你，**召回的内容里到底有多少是真金白银**。\n",
    "\n",
    "然后是**生成**（LLM）这一块。这里有个特别有意思的问题：模型会不会\"跑偏\"？就是明明给了它相关的上下文，它却自己发挥想象力去了。\n",
    "\n",
    "RAGAS 的**忠诚度**（Faithfulness）指标就是专门盯着这个的。之前用某个开源模型，忠诚度特别低，经常是给它 A，它能给你扯到 Z 去，真是让人哭笑不得。\n",
    "\n",
    "最后当然是看**整体效果**了——生成的答案到底对不对用户的胃口。**答案相关性**（Answer Relevance）这个指标就是最终的判官。\n",
    "\n",
    "其实做下来感觉，一个好的 RAG 系统就像一个优秀的研究助理：**检索要准**（高召回率），**筛选要精**（高上下文相关性），**理解要透**（高忠诚度），**表达要切题**（高答案相关性）。这四个维度缺一不可，任何一个短板都会拖累整体效果。\n",
    "\n",
    "不过说实话，实际操作中要把这几个指标都调优还真不容易，经常是按下葫芦浮起瓢，提高了召回率可能引入更多噪音，优化了忠诚度可能又损失了灵活性。这种平衡的艺术，大概就是做 RAG 系统最有挑战性的地方吧。\n",
    "\n",
    "## RAG 的重点在检索器的优化\n",
    "\n",
    "从 RAG 的实践中可以得出这样一个结论：**RAG 的灵魂在检索，不在生成**。\n",
    "\n",
    "怎么说呢？就像你去图书馆找资料，如果你连相关的参考文献都找不到，读书能力再强也写不出好论文啊。\n",
    "\n",
    "我们之前就吃过这个亏，用了个特别牛的大模型，结果效果还是很差。后来仔细一查，发现**检索这块儿根本就是个筛子，该捞的没捞上来**。\n",
    "\n",
    "提升召回率这事儿，我总结下来其实就是三板斧：\n",
    "\n",
    "第一板斧是**查询重写**。用户的问题往往**表达得很随意**，比如**问\"最新的报销政策\"** ，但知识库里可能写的是\"**2025 年差旅费用管理办法**\"。这时候你不改写查询，怎么可能检索到？\n",
    "\n",
    "我们试过让**大模型先理解用户意图**，然后**生成多个不同角度的查询**，效果立竿见影。有次一个同事问\"**出差住宿标准**\"，系统自动扩展成了\"**差旅住宿费用标准**\"、\"**员工出差酒店报销规定**\"等好几个查询，一下子把相关文档都捞出来了。\n",
    "\n",
    "第二板斧是**检索策略本身**。这里面门道就多了。\n",
    "\n",
    "比如说**文档怎么切分**？切太大，噪音多；切太小，上下文不全。我们试过各种**切分尺寸**（chunk size），最后发现还是得根据文档类型来。政策文件可以切大块，FAQ 就得切小点。\n",
    "\n",
    "还有就是要不要**用向量检索配合关键词检索**？纯向量检索有时候会漏掉一些专有名词，但关键词检索又太死板。最后我们搞了个**混合检索**，两边的结果都要，然后融合排序。\n",
    "\n",
    "第三板斧就是**重排序**了。初步检索可能捞上来几十个文档块，但真正相关的可能就那么几个。这时候需要一个更精准的模型来把真正有用的挑出来放在前面。我们用了个专门的**重排序模型**（reranker），这东西虽然慢点，但是准啊！\n",
    "\n",
    "给你看个真实的例子。我们有个同事要查\"**产假政策**\"，最开始系统召回率只有 40%左右，很多相关规定都没找到，比如\"**生育假期管理办法**\"、\"**女职工权益保护规定**\"这些。\n",
    "\n",
    "后来我们搞了**多路召回**，一路**用向量检索找语义相关的**，一路**用关键词检索找包含\"产假\"、\"生育\"、\"休假\"这些词的**，再加一路**用同义词扩展**。三路并行，召回率直接干到了 95%！\n",
    "\n",
    "最神奇的是，召回率上去之后，整个系统都活了。重排序因为有了更多高质量的候选文档，准确率从 70%提升到 92%。最终的问答准确率更是从 60%飙到 88%。这就是为啥我说**RAG 的重点在 R 不在 G**——检索不行，给再好的模型也是巧妇难为无米之炊。\n",
    "\n",
    "当然，这里面也有些坑。比如召回太多也不行，会引入噪音；重排序模型如果太重，延迟会很高。所以还是得根据实际场景找平衡。但总的来说，在检索上多花点功夫，绝对值得。\n",
    "\n",
    "那么，召回率和上下文相关性如何计算呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabfb88",
   "metadata": {},
   "source": [
    "# 召回率和上下文相关性的计算\n",
    "\n",
    "**上下文召回率**，说白了就是看你**该找到的东西有没有都找到**。打个比方，如果正确答案**需要 5 个关键信息点**，你的**检索系统找回来了 3 个**，那召回率就是 60%。数学公式虽然看着唬人，其实道理很简单——就是算个交集除以总数嘛。当然了，召回率越高，说明**检索组件召回的信息越全面**。\n",
    "\n",
    "**上下文相关性**，这个是看你**找回来的东西里，有多少是真正有用的**。我之前碰到过一个特别搞笑的例子，问\"**苹果手机的电池容量**\"，结果系统把关于\"**苹果营养价值**\"的内容也召回来了。虽然都有\"苹果\"这个词，但完全是两码事儿。这种情况下，相关性就很低了。同样的，上下文相关性越高，说明**检索组件检索到的上下文与查询的语义相关性越强**。\n",
    "\n",
    "实际操作的时候，LazyLLM 这个工具还挺好用的。你把问题、标准答案、检索到的内容，还有应该检索到的内容都准备好，几行代码就能算出这些指标。不过有个坑要注意——如果用 LLM 来评估的话，token 消耗会比较大，钱包要做好准备。\n",
    "\n",
    "我们通过一个简单案例模拟一下这两个指标的计算，直接上代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50fa35",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install lazyllm\n",
    "export QWEN_API_KEY='sk-这里填写你的key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "import os\n",
    "from lazyllm.tools.eval import LLMContextRecall, NonLLMContextRecall, ContextRelevance\n",
    "\n",
    "# 检索组件要求准备满足如下格式要求的数据进行评估\n",
    "data = [\n",
    "    {\n",
    "        \"question\": \"非洲的猴面包树果实的长度约是多少厘米？\",\n",
    "        # 当使用基于LLM的评估方法时要求答案是标注的正确答案\n",
    "        \"answer\": \"非洲猴面包树的果实长约15至20厘米。\",\n",
    "        # context_retrieved 为召回器召回的文档，按段落输入为列表\n",
    "        \"context_retrieved\": [\n",
    "            \"非洲猴面包树是一种锦葵科猴面包树属的大型落叶乔木，原产于热带非洲，它的果实长约15至20厘米。\",\n",
    "            \"钙含量比菠菜高50％以上，含较高的抗氧化成分。\",\n",
    "        ],\n",
    "        # context_reference 为标注的应当被召回的段落\n",
    "        \"context_reference\": [\n",
    "            \"非洲猴面包树是一种锦葵科猴面包树属的大型落叶乔木，原产于热带非洲，它的果实长约15至20厘米。\"\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "# 返回召回文档的命中率，例如上述data成功召回了标注的段落，因此召回率为1\n",
    "m_recall = NonLLMContextRecall()\n",
    "res = m_recall(data)  # 1.0\n",
    "print(res)\n",
    "\n",
    "# 返回召回文档中的上下文相关性分数，例如上述data召回的两个句子中只有一个是相关的\n",
    "m_cr = ContextRelevance()\n",
    "res = m_cr(data)  # 0.5\n",
    "print(res)\n",
    "\n",
    "# 返回基于LLM计算的召回率，LLM基于answer和context_retrieved判断是否召回了所有相关文档\n",
    "# 适用于没有标注的情况下使用，比较耗费 token ，请根据需求谨慎使用\n",
    "m_lcr = LLMContextRecall(\n",
    "    lazyllm.OnlineChatModule(\n",
    "        source=\"qwen\",\n",
    "        model=\"qwen-plus-latest\",\n",
    "        api_key=os.getenv(\"QWEN_API_KEY\"),\n",
    "    )\n",
    ")\n",
    "res = m_lcr(data)  # 1.0\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02e3f0",
   "metadata": {},
   "source": [
    "我们自己定义了一个非常简单的数据集。`context_retrieved`表示**实际被召回**的段落，`context_reference` 表示标注的**应当被召回**的段落。\n",
    "\n",
    "LazyLLM 在评估召回率上有两种方式，一种是`NonLLMContextRecall`方法，这种情况不调用大语言模型计算，由于`context_retrieved` 包含了 `context_reference` 中的所有内容，因此所有相关的上下文都被成功召回了，召回率为 1。\n",
    "\n",
    "另一种方式是`LLMContextRecall`方法，这种情况会调用大语言模型，基于`answer`和`context_retrieved`判断是否召回了所有相关文档。大致流程是这样的：\n",
    "\n",
    "1. 输入数据 ：需要包含 `question` （问题）、 `answer` （标准答案）和 `context_retrieved` （检索器召回的文档）字段。\n",
    "\n",
    "2. 处理过程 ：\n",
    "\n",
    "   1. 将所有召回的文档内容连接成一个字符串作为上下文。\n",
    "   2. 构造一个查询字符串，包含问题、上下文和标准答案。\n",
    "   3. 使用大语言模型（LLM）对这个查询进行评估，判断答案中的每个句子是否可以归因于给定的上下文。\n",
    "\n",
    "3. 评估逻辑 ：\n",
    "\n",
    "   1. LLM 会分析答案中的每个句子，并给出二元评分（1 表示完全支持，0 表示不支持/矛盾）。评估结果是一个包含每个句子评分的列表。\n",
    "\n",
    "4. 计算召回率 ：计算所有句子评分的平均值，即为最终的召回率。\n",
    "\n",
    "同样由于 `context_retrieved` 包含了 `context_reference` 的所有内容，因此 LLM 判断所有句子都得到了支持，召回率为 1.0。\n",
    "\n",
    "计算召回率 ：\n",
    "\n",
    "上下文相关性分数`ContextRelevance`就更好理解了，`context_retrieved`里面与问答相关的只有一句话，所以分数为 0.5。\n",
    "\n",
    "当然了，这个数据集是我们自定义的。如果使用真实的数据集呢？我们用第二节实现的基于 [cmrc2018](https://huggingface.co/datasets/LazyAGI/CMRC2018_Knowledge_Base/tree/main) 数据集做的 RAG 系统进行评测，以下是评测代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc207671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import lazyllm\n",
    "import os\n",
    "# 导入评估工具：LLMContextRecall（基于大语言模型的上下文召回率）、NonLLMContextRecall（非大语言模型的上下文召回率）、ContextRelevance（上下文相关性）\n",
    "from lazyllm.tools.eval import LLMContextRecall, NonLLMContextRecall, ContextRelevance\n",
    "# 导入Hugging Face datasets库，用于加载数据集\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 加载cmrc2018数据集，指定缓存目录为当前目录下的datasets文件夹\n",
    "dataset = load_dataset(\"cmrc2018\", cache_dir=\"./datasets\")\n",
    "\n",
    "\n",
    "def create_evaluation_data(test_dataset, topk_values=[1, 3, 5]):\n",
    "    \"\"\"创建用于评估的数据结构\n",
    "\n",
    "    Args:\n",
    "        test_dataset: 测试数据集\n",
    "        topk_values: 检索器返回的topk值（召回的文档数）列表，默认为[1, 3, 5]\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含不同topk值评估数据的字典\n",
    "    \"\"\"\n",
    "    # 文档加载，将data_kb目录作为知识库\n",
    "    documents = lazyllm.Document(dataset_path=\"./data_kb\")\n",
    "\n",
    "    # 初始化评估数据字典\n",
    "    evaluation_data = {}\n",
    "\n",
    "    # 为每个topk值创建评估数据\n",
    "    for topk in topk_values:\n",
    "        # 检索组件定义\n",
    "        # doc: 指定文档对象\n",
    "        # group_name: 指定文档分组方式为\"CoarseChunk\"(粗粒度分块)\n",
    "        # similarity: 使用\"bm25_chinese\"算法计算相似度，适合中文检索\n",
    "        # topk: 返回最相关的topk个结果\n",
    "        retriever = lazyllm.Retriever(\n",
    "            doc=documents, group_name=\"CoarseChunk\", similarity=\"bm25_chinese\", topk=topk\n",
    "        )\n",
    "\n",
    "        # 初始化数据列表\n",
    "        data = []\n",
    "        # 使用测试集的前10个样本进行评估\n",
    "        for i in range(min(10, len(test_dataset))):\n",
    "            # 获取当前样本\n",
    "            sample = test_dataset[i]\n",
    "            # 提取问题\n",
    "            question = sample[\"question\"]\n",
    "            # 提取答案，取第一个答案作为标准答案\n",
    "            answer = sample[\"answers\"][\"text\"][0]\n",
    "            # 提取标注的应当被召回的段落\n",
    "            context_reference = [sample[\"context\"]]\n",
    "\n",
    "            # 获取检索器召回的文档\n",
    "            doc_node_list = retriever(query=question)\n",
    "            # 提取召回文档的内容\n",
    "            context_retrieved = [node.get_content() for node in doc_node_list]\n",
    "\n",
    "            # 将数据添加到列表中\n",
    "            data.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"context_retrieved\": context_retrieved,\n",
    "                \"context_reference\": context_reference,\n",
    "            })\n",
    "\n",
    "        # 将当前topk值的数据添加到评估数据字典中\n",
    "        evaluation_data[topk] = data\n",
    "\n",
    "    return evaluation_data\n",
    "\n",
    "\n",
    "def evaluate_rag_performance(evaluation_data):\n",
    "    \"\"\"评估RAG性能\n",
    "\n",
    "    Args:\n",
    "        evaluation_data: 包含不同topk值评估数据的字典\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含不同topk值评估结果的字典\n",
    "    \"\"\"\n",
    "    # 初始化评估工具\n",
    "    # NonLLMContextRecall: 计算召回文档的命中率\n",
    "    m_recall = NonLLMContextRecall()\n",
    "    # ContextRelevance: 计算召回文档中的上下文相关性分数\n",
    "    m_cr = ContextRelevance()\n",
    "    # LLMContextRecall: 计算基于LLM的召回率\n",
    "    m_lcr = LLMContextRecall(\n",
    "        lazyllm.OnlineChatModule(\n",
    "            source=\"qwen\",\n",
    "            model=\"qwen-plus-latest\",\n",
    "            api_key=os.getenv(\"QWEN_API_KEY\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 初始化结果字典\n",
    "    results = {}\n",
    "\n",
    "    # 对每个topk值进行评估\n",
    "    for topk, data in evaluation_data.items():\n",
    "        print(f\"\\n=== TopK = {topk} 评估结果 ===\")\n",
    "\n",
    "        # 计算召回文档的命中率\n",
    "        recall_score = m_recall(data)\n",
    "        print(f\"召回率 (Recall): {recall_score}\")\n",
    "\n",
    "        # 计算召回文档中的上下文相关性分数\n",
    "        relevance_score = m_cr(data)\n",
    "        print(f\"上下文相关性 (Context Relevance): {relevance_score}\")\n",
    "\n",
    "        # 计算基于LLM的召回率\n",
    "        try:\n",
    "            llm_recall_score = m_lcr(data)\n",
    "            print(f\"基于LLM的召回率 (LLM Context Recall): {llm_recall_score}\")\n",
    "        except Exception as e:\n",
    "            print(f\"计算基于LLM的召回率时出错: {e}\")\n",
    "            llm_recall_score = 0.0\n",
    "\n",
    "        # 将结果添加到结果字典中\n",
    "        results[topk] = {\n",
    "            \"recall\": recall_score,\n",
    "            \"relevance\": relevance_score,\n",
    "            \"llm_recall\": llm_recall_score\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 创建评估数据\n",
    "    print(\"正在创建评估数据...\")\n",
    "    evaluation_data = create_evaluation_data(dataset[\"test\"], topk_values=[1, 3, 5])\n",
    "\n",
    "    # 评估RAG性能\n",
    "    print(\"开始评估RAG性能...\")\n",
    "    results = evaluate_rag_performance(evaluation_data)\n",
    "\n",
    "    # 打印最终结果\n",
    "    print(\"\\n=== 最终评估结果 ===\")\n",
    "    for topk, scores in results.items():\n",
    "        print(f\"\\nTopK = {topk}:\")\n",
    "        print(f\"  召回率: {scores['recall']:.4f}\")\n",
    "        print(f\"  上下文相关性: {scores['relevance']:.4f}\")\n",
    "        print(f\"  基于LLM的召回率: {scores['llm_recall']:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa517d6f",
   "metadata": {},
   "source": [
    "我看了下 [cmrc2018](https://huggingface.co/datasets/LazyAGI/CMRC2018_Knowledge_Base/tree/main) 数据集的测试结果，还是很满意的。\n",
    "\n",
    "| TopK 值 | 召回率 | 上下文相关性 | 基于 LLM 的召回率 |\n",
    "| ------- | ------ | ------------ | ----------------- |\n",
    "| 1       | 0.9000 | 1.0000       | 1.0000            |\n",
    "| 3       | 1.0000 | 0.3567       | 0.9000            |\n",
    "| 5       | 1.0000 | 0.2161       | 0.9000            |\n",
    "\n",
    "不知道有没有发现这样一个现象，随着召回文档数量增加，相关性反而从 100%掉到 21.6%了。这就像是**为了找一个答案，把整个图书馆的书都搬过来，结果真正有用的反而被淹没了**。\n",
    "\n",
    "这种情况我太熟悉了。刚开始做 RAG 的时候，总想着多召回一些文档总没错吧？结果发现不是这么回事儿。**垃圾信息太多，大模型也会被带偏**。有一次我们的系统，用户问\"如何做西红柿炒蛋\"，结果因为召回了太多无关内容，最后回答里居然出现了\"西红柿的种植技术\"...\n",
    "\n",
    "所以现在做 RAG，不能光追求召回率，相关性同样重要。这就像做菜，不是食材越多越好，而是要精准。宁可少而精，也不要多而杂。当然，怎么在两者之间找平衡，这又是另一个让人头疼的问题了。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
